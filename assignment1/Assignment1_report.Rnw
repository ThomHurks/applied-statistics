\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{placeins}
\usepackage{longtable,framed}
\usepackage{amsmath}

<<echo=FALSE>>=
library(sas7bdat)
dataset = read.sas7bdat("assignment1.sas7bdat")
batchB0 = dataset[dataset$BATCH == "B0",]
batchB1 = dataset[dataset$BATCH == "B1",]
batchB2 = dataset[dataset$BATCH == "B2",]
batchB3 = dataset[dataset$BATCH == "B3",]
batchB4 = dataset[dataset$BATCH == "B4",]
batchB5 = dataset[dataset$BATCH == "B5",]
batchB6 = dataset[dataset$BATCH == "B6",]
batchB7 = dataset[dataset$BATCH == "B7",]
batchB1toB7 = dataset[dataset$BATCH != "B0",]
dataset$LOGOUTCOME = log(dataset$OUTCOME)
@

\begin{document}
\title{Analysis of silicone contamination of batches}

\author{
\begin{minipage}{0.6\textwidth}
\centering
\begin{tabular}{l r}
Thom Hurks & \textit{0828691}\\
Puck Mulders & \textit{0737709}\\
Marijn van Knippenberg  & \textit{0676548}\\
Rik Coenders & \textit{0777004}
\end{tabular}
\end{minipage}
}

\date{\today}

\maketitle
\tableofcontents

\section{Introduction}
The health inspection has detected a silicone leakage in a machine used in a chemical production process. The leakage was detected in close vicinity of packed product, so the question is whether the product has been affected by the leakage. The machines are regularly inspected, and data on a batch produced before the previous inspection is available and is denoted as batch B0. Batches denoted B1 to B7 have been produced between the previous inspection and the current inspection and may have been affected by the leakage. The data consists of measurements of samples of units from the batches. The silicone that leaked is already part of the product itself, so it will always be present in the measured units to some extent. The goal of this report is to possibly identify the batches that were affected by the silicone leakage.
\\\\ 
In the first section we describe which methods we use to find the affected batches. In the next section we will describe what the results of these methods are. Lastly, we will conclude which of the batches are affected. In this section we will also describe what the weaknesses and risks are of our investigation.
\newpage
\section{Methods}
To decide which of the batches are affected, if any, we perform both parametric and non-parametric tests. The parametric tests assume that the data is normally distributed, which means we must test for normality and if necessary transform the data. Non-parametric tests are generally less powerful than parametric tests, but can always be applied regardless of normality. With parametric tests one must be careful to draw conclusions about the original non-transformed data if the data needs to be transformed in order to meet the normality assumption. As a start, we will plot the data and perform a Shapiro-Wilk test to see whether the non-transformed data is normal. After that, we will perform a Wald-Wolfowitz Runs Test which has as a null hypothesis that each element in the sequence is independently drawn from the same random distribution; if this is not the case, that could be an indicator that some of the batches are affected by silicones. We reject the null hypothesis if $p < 0.01$.
\subsection{Critical value}
Most of our hypothesis include multiple comparisons. Because of this reason, we have to adjust the critical value to make sure that it will not reject batches too soon. Applying Bonferroni will give us an $\alpha=\frac{0,05}{7}=0,00714$, but since this value is quite low, it can be the case that too many batches will pass the test. Because we are dealing with dangerous materials, we will raise the alpha value slightly to prevent batches from passing the test even though they could be infected. To be a bit more strict, we will set the critical value to $0.01$.
\subsection{Normal distribution}
To decide whether the data is normally distributed, we will plot the data to get an insight in how the data is distributed. We will then perform the Shapiro-Wilk tests on each of the batches to check whether we can reject the null hypothesis that the data comes from a normal distribution. We will perform a skewness and kurtosis test to see whether batch 0 is in the appropriate range of skewness (between -0,3 and 0,3) and in the appropriate range of kurtosis (between -0,5 and 1,5). $\alpha=0,05$ is chosen as the critical value. If we can indeed conclude that the batches are normally distributed, we can compare each batch with batch 0 using a t-test. If, however, the data is not normally distributed, we may apply a transformation to remove skewness and bring the data closer to a normal distribution. If a transformation is necessary, we must be careful to draw conclusions about the original untransformed data, however. Because batch 0 is produced before the leakage was discovered, we know that the mean and variance of the amount of silicones found in each batch should be similar to the amount of silicone in batch 0. Therefore our hypothesis is the following:
\begin{align}
\begin{cases}
H_0: \mu_0 = \mu_i & \quad \text{for} 1 \leq i \leq 7 \\
H_1: \mu_0 \neq \mu_i & \quad \text{for} 1 \leq i \leq 7
\end{cases}
\end{align}
If the result of the t-test is less than $0.01$, we will reject the null hypothesis. \\\\
Besides performing a t-test, we will also perform some homogeneity tests to see whether the data also share the same characteristics. To check if this is the case, we will perform a Barlett's test, which checks whether all the variances of the batches are equal to each other. The null hypothesis of that test is that all variances are equal, and thus that the sets are homogeneous. We will also perform an F-test to see which batches have similar properties as batch 0.
\subsection{Non-parametric tests}
We will perform a serial correlation test on the means of each batches. The serial correlation test is Von Neumann with lag-1 autocorrelation. As a null hypothesis we will assume that the means are random. If they are random, than there will be no clear pattern. This is expected if no of the batches will be affected. We will reject the null hypothesis is $p<0.05$.
\section{Results}
\subsection{QQ Plot and Shapiro-Wilk}
The first step is to generate a normal QQ plot for batch 0 as an easy visual confirmation of normality.
<<figure1, fig.cap="QQ Plot of batch 0", fig.show="asis", fig.pos = "h!", echo=FALSE>>=
qqnorm(batchB0$OUTCOME)
qqline(batchB0$OUTCOME)
swtestB0 = shapiro.test(batchB0$OUTCOME)
@
\FloatBarrier
The normal QQ plot already seems to suggest that batch 0 is not normally distributed. When we also perform the Shapiro-Wilk normality test we can see the p-value $= \Sexpr{swtestB0$p.value} < 0.05$ so we can reject the null-hypothesis that the outcome is normally distributed.\\\\
Next we can generate a normal QQ plot for the other, possible affected, batches 1 to 7. Of course generating this plot for multiple batches at once assumes that the units are somewhat consistent across different batches.
<<figure2, fig.cap="QQ Plot of batches 1 to 7", fig.show="asis", fig.pos = "h!", echo=FALSE>>=
qqnorm(batchB1toB7$OUTCOME)
qqline(batchB1toB7$OUTCOME)
swtestB0toB7 = shapiro.test(batchB1toB7$OUTCOME)
@
\FloatBarrier
The normal QQ plot here too seems to suggest that the data is not normally distributed. When we again perform the Shapiro-Wilk normality test we can see the p-value $= \Sexpr{swtestB0toB7$p.value} < 0.05$ so we can reject the null-hypothesis that the outcome of batches 1 to 7 is normally distributed.\\\\
\subsection{Normal distribution}
\subsubsection{Shapiro-Wilk on log-transformed data}
In the previous section we observed that the data does not seem to follow a normal distribution. In order to perform parametric tests that assume normality, we must then transform the data. We will transform the data using the well-known log transformation and then check for each batch what the p-value of the Shapiro-Wilk test is in order to confirm that the transformation had the desired effect.
<<echo=FALSE, results="asis">>=
library(xtable)
sw_log_batchB0 = shapiro.test(log(batchB0$OUTCOME))$p.value
sw_log_frame = data.frame(batch=c("B0", "B1", "B2", "B3", "B4", "B5", "B6", "B7"), 
                          "p-value"=c(sw_log_batchB0, shapiro.test(log(batchB1$OUTCOME))$p.value,
                                      shapiro.test(log(batchB2$OUTCOME))$p.value, shapiro.test(log(batchB3$OUTCOME))$p.value,
                                      shapiro.test(log(batchB4$OUTCOME))$p.value, shapiro.test(log(batchB5$OUTCOME))$p.value,
                                      shapiro.test(log(batchB6$OUTCOME))$p.value, shapiro.test(log(batchB7$OUTCOME))$p.value))
xtable(sw_log_frame)
@
No of the batches reject normality, so using the Shapiro-Wilk test we cannot reject normality for all of the batches. 
We can generate a normal QQ-plot to get more insight into the log transformed batch 0.
<<figure3, fig.cap="QQ Plot of the log of batch 0", fig.show="asis", fig.pos = "h!", echo=FALSE>>=
qqnorm(log(batchB0$OUTCOME))
qqline(log(batchB0$OUTCOME))
@
\FloatBarrier
\subsubsection{Skewness and Kurtosis}
<<echo=FALSE, warning=FALSE, message=FALSE, results="asis">>=
library(e1071)

batches = unique(dataset$BATCH);
skewness = list();
kurtosis = list();

for(batch in batches) {
  Bi = dataset[dataset$BATCH == batch,]$LOGOUTCOME
  skewness[batch] <-skewness(Bi);
  kurtosis[batch] <-kurtosis(Bi);
}
kurtosis_skewness = data.frame(
                              "kurtosis"= unlist(kurtosis), "skewness" = unlist(skewness))
library(xtable)
xtable(kurtosis_skewness)
@
\FloatBarrier
<<echo=FALSE, warning=FALSE, message=FALSE, results="asis">>=
library(moments)

batches = unique(dataset$BATCH);
jarqueje = list();

for(batch in batches) {
  Bi = dataset[dataset$BATCH == batch,]$LOGOUTCOME
  jarqueje[batch] <- as.numeric(unlist(jarque.test(Bi))["p.value"])
}
jarque_bera = data.frame(
                              "Jarque-Bera"= unlist(jarqueje))
library(xtable)
xtable(jarque_bera)
@
After performing the Shapiro-Wilk test, we have performed skewness and kurtosis tests. We can compute the skewness and kurtosis metrics and observe that for batch 0 they are respectively. This means that although the kurtosis is in the appropriate range to be normally distributed, the skewness of the data is a bit too high. This kind of test is also formalized in the Jarque-Bera test, which tests if the skewness is close to zero and the kurtosis close to three. The p-value of the Jarque-Bera test is , so it cannot reject the normality hypothesis based on skewness and kurtosis. The data is skewed right, which is probably the reason that the Shapiro-Wilk test rejects the assumption of a normal distribution. The p-value of the Shapiro-Wilk test is not very low and the skewness, kurtosis and Jarque-Bera tests seem to suggest a close-to-normal shape. In combination with the fact that for all the other batches we cannot reject the normality null hypothesis, we will assume that batch 0 is normally distributed for the following tests. It must also be noted that the t-test is robust against data that deviates somewhat from normality, so our assumption at least for that test is reasonable.
\subsubsection{T-Test}
We can use a t-test to compare the batches, since we could not reject the null hypothesis that the log-transformed batches are normally distributed.
To see which t-test variant we need to perform, we need to analyse the variance of all the log-transformed batches.
<<echo=FALSE, warning=FALSE, message=FALSE, results="asis">>=
var_log_B7 = var(log(batchB7$OUTCOME[!is.nan(batchB7$OUTCOME)]))
variance_log_frame = data.frame(batch=c("B0", "B1", "B2", "B3", "B4", "B5", "B6", "B7"), 
                              "variance"=c(var(log(batchB0$OUTCOME)), var(log(batchB1$OUTCOME)), var(log(batchB2$OUTCOME)), var(log(batchB3$OUTCOME)),
                                           var(log(batchB4$OUTCOME)), var(log(batchB5$OUTCOME)), var(log(batchB6$OUTCOME)), var_log_B7))
library(xtable)
xtable(variance_log_frame)
@
The variance differs between batches, as can be seen in the preceding table. Because of that, we will perform a t-test that assumes unequal variances. It must be noted that batch 7 has one missing value.\\
The results of the t-tests can be found in the following table:
<<echo=FALSE, warning=FALSE, message=FALSE, results="asis">>=
ttest_B1 <- t.test(log(batchB0$OUTCOME), log(batchB1$OUTCOME), var.equal = FALSE)$p.value
ttest_B2 <- t.test(log(batchB0$OUTCOME), log(batchB2$OUTCOME), var.equal = FALSE)$p.value
ttest_B3 <- t.test(log(batchB0$OUTCOME), log(batchB3$OUTCOME), var.equal = FALSE)$p.value
ttest_B4 <- t.test(log(batchB0$OUTCOME), log(batchB4$OUTCOME), var.equal = FALSE)$p.value
ttest_B5 <- t.test(log(batchB0$OUTCOME), log(batchB5$OUTCOME), var.equal = FALSE)$p.value
ttest_B6 <- t.test(log(batchB0$OUTCOME), log(batchB6$OUTCOME), var.equal = FALSE)$p.value
ttest_B7 <- t.test(log(batchB0$OUTCOME), log(batchB7$OUTCOME), var.equal = FALSE)$p.value
ttest_log_frame = data.frame(batch=c("B1", "B2", "B3", "B4", "B5", "B6", "B7"), 
                              "p-value"=c(ttest_B1, ttest_B2, ttest_B3, ttest_B4, ttest_B5, ttest_B6, ttest_B7))
library(xtable)
xtable(ttest_log_frame)
@
The results indicate that batches two, three and seven have a mean that is different from the mean of the unaffected batch, since their p-values are less than $0.05$. This could indicate that these batches are affected.
\subsubsection{Homogeneity tests}
Performing the Barlett's tests gives us a p-value of $4,523*10^{-15}$. This rejects the null hypothesis that all the batches have the same variance. After performing an F-test that compares all the batches with batch 0, we have found the following results:
\begin{center}
\begin{tabular}{| l | l |}
\hline
Batch & P-value \\ \hline 
B1 & 0.04429337 \\ \hline 
B2 &  0.003227914 \\ \hline 
B3 & 0.06260923\\ \hline 
B4 & $4.570405*10^{-7}$  \\ \hline 
B5 & 0.08812147  \\ \hline 
B6 & 0.266533  \\ \hline 
B7 & 0.9583088 \\ 
\hline 
\end{tabular}
\end{center}
These results indicate that batch 1, 2, and 4 have different properties than batch 0. 
\subsection{Non-parametric tests}
\subsubsection{Serial correlation tests with means}
We have performed a Rank von Neumann Test for lag-1. The p-value is $0.4895337 < 0.05$ and we therefore have to reject the null hypothesis. This means that the serial correlation test detects some pattern in the means of the batches. 
\subsubsection{Wald-Wolfowitz Runs Test}
<<echo=FALSE, message=FALSE, warning=FALSE, results="hide">>=
library(adehabitatLT)
wawo <- wawotest(dataset$OUTCOME)
@
We have performed a Wald-Wolfowitz Runs test. We see that the p-value $=\Sexpr{wawo["p"]} < 0.05$ so we can reject that null hypothesis. This is an interesting result, because we know the data is obtained from the same source, but part of the samples may or may not be tainted, and this result may point to that being true.\\
\subsubsection{Rank Serial Correlation Test}
Here we perform the rank serial autocorrelation test at lag 1 using the rank von Neumann ratio. This tests the null hypothesis that the lag-$k$ autocorrelation is $0$ for all values of $k$ greater than $0$ (i.e., the time series is purely random). The procedure that we use emits some warnings because the dataset contains ties, and with this specific test that can make the p-value less accurate. With that warning in mind, we present the table containing the rank serial autocorrelation test p-values of each batch:\\
<<echo=FALSE, warning=FALSE, message=FALSE, results="asis">>=
library(EnvStats)
serialCorB0 = unlist(serialCorrelationTest(batchB0$OUTCOME[!is.nan(batchB0$OUTCOME)], test = "rank.von.Neumann")["p.value"])
serialCorB1 = unlist(serialCorrelationTest(batchB1$OUTCOME[!is.nan(batchB1$OUTCOME)], test = "rank.von.Neumann")["p.value"])
serialCorB2 = unlist(serialCorrelationTest(batchB2$OUTCOME[!is.nan(batchB2$OUTCOME)], test = "rank.von.Neumann")["p.value"])
serialCorB3 = unlist(serialCorrelationTest(batchB3$OUTCOME[!is.nan(batchB3$OUTCOME)], test = "rank.von.Neumann")["p.value"])
serialCorB4 = unlist(serialCorrelationTest(batchB4$OUTCOME[!is.nan(batchB4$OUTCOME)], test = "rank.von.Neumann")["p.value"])
serialCorB5 = unlist(serialCorrelationTest(batchB5$OUTCOME[!is.nan(batchB5$OUTCOME)], test = "rank.von.Neumann")["p.value"])
serialCorB6 = unlist(serialCorrelationTest(batchB6$OUTCOME[!is.nan(batchB6$OUTCOME)], test = "rank.von.Neumann")["p.value"])
serialCorB7 = unlist(serialCorrelationTest(batchB7$OUTCOME[!is.nan(batchB7$OUTCOME)], test = "rank.von.Neumann")["p.value"])
serial_cor_frame = data.frame(batch=c("B0", "B1", "B2", "B3", "B4", "B5", "B6", "B7"), 
                              "p-value"=c(serialCorB0, serialCorB1, serialCorB2, serialCorB3,
                                            serialCorB4, serialCorB5, serialCorB6, serialCorB7))
library(xtable)
xtable(serial_cor_frame)
@
As one can observe, the p-values are all large, which means that we cannot reject the null hypothesis that the batches are random using this test.
\section{Conclusion}

\end{document}
