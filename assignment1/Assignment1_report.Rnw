\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{placeins}
\usepackage{longtable,framed}
\usepackage{amsmath,amssymb}

<<echo=FALSE>>=
library(sas7bdat)
dataset = read.sas7bdat("assignment1.sas7bdat")
batchB7full = dataset[dataset$BATCH == "B7",]
batchB7mean = batchB7full
batchB7mean[7,3] = 4.49
dataset = dataset[!is.nan(dataset$OUTCOME),]
batchB0 = dataset[dataset$BATCH == "B0",]
batchB1 = dataset[dataset$BATCH == "B1",]
batchB2 = dataset[dataset$BATCH == "B2",]
batchB3 = dataset[dataset$BATCH == "B3",]
batchB4 = dataset[dataset$BATCH == "B4",]
batchB5 = dataset[dataset$BATCH == "B5",]
batchB6 = dataset[dataset$BATCH == "B6",]
batchB7 = dataset[dataset$BATCH == "B7",]
batchB1toB7 = dataset[dataset$BATCH != "B0",]
dataset$LOGOUTCOME = log(dataset$OUTCOME)
@

\begin{document}
\title{Analysis of silicone contamination of batches}

\author{
\begin{minipage}{0.6\textwidth}
\centering
\begin{tabular}{l r}
Thom Hurks & \textit{0828691}\\
Puck Mulders & \textit{0737709}\\
Marijn van Knippenberg  & \textit{0676548}\\
Rik Coenders & \textit{0777004}
\end{tabular}
\end{minipage}
}

\date{\today}

\maketitle
\tableofcontents

\section{Introduction}
The health inspection has detected a silicone leakage in a machine used in a chemical production process. The leakage was detected in close vicinity of packed product, so the question is whether the product has been affected by the leakage. The machines are regularly inspected, and data on a batch produced before the previous inspection is available and is denoted as batch B0. Batches denoted B1 to B7 have been produced between the previous inspection and the current inspection and may have been affected by the leakage. The data consists of measurements of samples of units from the batches. The silicone that leaked is already part of the product itself, so it will always be present in the measured units to some extent. The goal of this report is to possibly identify the batches that were affected by the silicone leakage.
\\\\ 
In the first section we describe which methods we use to find the affected batches. In the next section we will describe what the results of these methods are. Lastly, we will conclude which of the batches are affected. In this section we will also describe what the weaknesses and risks are of our investigation.
\newpage
\section{Methods}
To decide which of the batches are affected, if any, we perform both parametric and non-parametric tests. The parametric tests assume that the data is normally distributed, which means we must test for normality and if necessary transform the data. Non-parametric tests are generally less powerful than parametric tests, but can always be applied regardless of normality. With parametric tests one must be careful to draw conclusions about the original non-transformed data if the data needs to be transformed in order to meet the normality assumption. As a start, we will plot the data and perform a Shapiro-Wilk test to see whether the non-transformed data is normal. After that, we will perform a Wald-Wolfowitz Runs Test which has as a null hypothesis that each element in the sequence is independently drawn from the same random distribution; if this is not the case, that could be an indicator that some of the batches are affected by silicones. We reject the null hypothesis if $p < 0.01$.
\subsection{Critical value}
Most of our hypothesis include multiple comparisons. Because of this reason, we have to adjust the critical value to make sure that it will not reject batches too soon. Applying Bonferroni will give us an $\alpha=\frac{0,05}{7}=0,00714$, but since this value is quite low, it can be the case that too many batches will pass the test. Because we are dealing with dangerous materials, we will raise the alpha value slightly to prevent batches from passing the test even though they could be infected. To be a bit more strict, we will set the critical value to $0.01$.
\subsection{Normal distribution}
To decide whether the data is normally distributed, we will plot the data to get an insight in how the data is distributed. We will then perform the Shapiro-Wilk tests on each of the batches to check whether the data is now normally distributed. We formulate the hypothesis as following:
\begin{align}
\label{normal}
\begin{cases}
H_0: B_i \sim N & \quad \text{for} 1 \leq i \leq 7 \\
H_1: B_i\nsim N
\end{cases}
\end{align}
We will reject $H_0$ if $p < 0,01$. Besides a Shapiro-Wilk test, we will perform skwewness and kurtosis tests to test on where normality is violated. The values for skewness should be between -0,03 and 0,3, while the values of kurtosis should be between -0,5 and 1,5. We will perform a Jarque-Bera test to see if it is appropriate to reject a normal distribution based on skewness and kurtosis. The null hypothesis is the same as with the Shapiro-Wilk test, which can be found in equation \ref{normal}.
If, however, the data is not normally distributed, we will apply a transformation to remove skewness and bring the data closer to a normal distribution. If a transformation is necessary, we must be careful to draw conclusions about the original untransformed data. Because batch 0 is produced before the previous inspection where no leakage was discovered, we know that the mean and variance of the amount of silicones found in each batch should be similar to the amount of silicone in batch 0. To check whether they are similar, we will perform a one-sided t-test, because a leakage will cause the outcome to become higher, while it would be acceptable if the average amount of silicones in a batch is lower than in batch 0. Therefore the hypothesis is the following:
\begin{align}
\label{means}
\begin{cases}
H_0: \mu_0 \leq \mu_i & \quad \text{for} 1 \leq i \leq 7 \\
H_1: \mu_0 < \mu_i & \quad \text{for} 1 \leq i \leq 7
\end{cases}
\end{align}
If the result of the t-test is less than $0.01$, we will reject the null hypothesis. \\\\
Besides performing a t-test, we will also perform some homogeneity tests to see whether the data also share the same characteristics. To check if this is the case, we will perform a Barlett's test, which checks whether all the variances of the batches are equal to each other. The null hypothesis of that test is that all variances are equal, and thus that the sets are homogeneous. We will also perform an F-test to see which batches have similar properties as batch 0.
\subsection{Non-parametric tests}
\subsubsection{Serial correlation test on the means}
We will perform a serial correlation test on the means of the batches. The serial correlation test is Von Neumann with lag-1 autocorrelation. The null hypothesis is that the means of the batches are random. If this indeed is the case, this is an indication that none of the batches is affected by the silicones. We will reject the null hypothesis if $p<0.05$.
\subsubsection{Kolmogorov-Smirnov Test}
If the non-transformed data does not follow a normal distribution, we will also perform some non-parametric tests that do not assume a normal distribution. Such a parametric test is the Kolgomorov-Smirnov Test. For the two-sample version of this test, we need to make the following assumptions:
\begin{itemize}
  \item The two samples are independent.
  \item The outcomes are ordinal or numerical.
\end{itemize}
Since the results of the samples do not depend on each other, the first assumption can be made. Second, the outcomes are all numerical so the second assumption can be made too. We perform a two-sided test to get a general view of wether non-parametric testing picks up on any differiations between the distributions of batch 0 and the other batches. Our hypothesis is the following:
\begin{align}
\begin{cases}
H_0: F_0 \leq F_i & \quad \text{for} 1 \leq i \leq 7 \\
H_1: F_0 \neq F_i & \quad \text{for} 1 \leq i \leq 7
\end{cases}
\end{align}
We will reject the null hypothesis when $pv-value < 0,01$. Because batch 7 has a missing value, we have decided to impute the mean of batch 7 into the line with the missing value, to see whether this affects the test.
\subsubsection{Wilcoxon Rank-Sum Test}
A WILCOXON SIGNED RANK TEST IS UITGEVOERD EN NIET EEN WILCOXON RANK SUM TEST DUS DAT MOET VERANDERD WORDEN. HIER MOET OOK EEN NULL HYPOTHESE VOOR GEMAAKT WORDEN. VERANDEREN DUS!
To compare w
Another test to compare the clean batch with the other batches without any assumption of normality is the Wilcoxon Signed-Rank Test. To apply this test, the following assumptions are made:

\begin{itemize}
  \item The samples have been collected randomly from their respective populations.
  \item Sample pairs are independent.
  \item The observed variable is a continuous random variable.
  \item The underlying distributions from which the samples are derived are indentical in shape. 
\end{itemize}

We can make the first two assumptions based on the assignment description. Although the measurements themselves have limited precision, the underlying value is continuous, which justifies the third assumption. For the last assumption, we refer to the similarity of the histograms of the different batches. 

Contamination is caused by a leakage, so we expect tainted batches to generally have higher sample values. We perform a one-sided Wilcoxon Signed-Rank Test with the clean batch and each of the potentially tainted batches. 
\subsection{Outliers}
We will perform a Grubbs outlier test to check whether 
\section{Results}
\subsection{QQ Plot and Shapiro-Wilk}
The first step is to generate a normal QQ plot for batch 0 as an easy visual confirmation of normality.
<<figure1, fig.cap="QQ Plot of batch 0", fig.show="asis", fig.pos = "h!", echo=FALSE>>=
qqnorm(batchB0$OUTCOME)
qqline(batchB0$OUTCOME)
swtestB0 = shapiro.test(batchB0$OUTCOME)
@
\FloatBarrier
The normal QQ plot already seems to suggest that batch 0 is not normally distributed. When we also perform the Shapiro-Wilk normality test we can see the p-value $= \Sexpr{swtestB0$p.value} < 0.01$ so we can reject the null-hypothesis that the outcome is normally distributed.\\\\
Next we can generate a normal QQ plot for the other, possible affected, batches 1 to 7. Of course generating this plot for multiple batches at once assumes that the units are somewhat consistent across different batches.
<<figure2, fig.cap="QQ Plot of batches 1 to 7", fig.show="asis", fig.pos = "h!", echo=FALSE>>=
qqnorm(batchB1toB7$OUTCOME)
qqline(batchB1toB7$OUTCOME)
swtestB0toB7 = shapiro.test(batchB1toB7$OUTCOME)
@
\FloatBarrier
The normal QQ plot here too seems to suggest that the data is not normally distributed. When we again perform the Shapiro-Wilk normality test we can see the p-value $= \Sexpr{swtestB0toB7$p.value} < 0.01$ so we can reject the null-hypothesis that the outcome of batches 1 to 7 is normally distributed.\\\\
\subsection{Normal distribution}
\subsubsection{Shapiro-Wilk on log-transformed data}
In the previous section we observed that the data does not seem to follow a normal distribution. In order to perform parametric tests that assume normality, we must then transform the data. We will transform the data using the well-known log transformation and then check for each batch what the p-value of the Shapiro-Wilk test is in order to confirm that the transformation had the desired effect.
<<echo=FALSE, results="asis">>=
library(xtable)
sw_log_batchB0 = shapiro.test(log(batchB0$OUTCOME))$p.value
sw_log_frame = data.frame(batch=c("B0", "B1", "B2", "B3", "B4", "B5", "B6", "B7"), 
                          "p-value"=c(sw_log_batchB0, shapiro.test(log(batchB1$OUTCOME))$p.value,
                                      shapiro.test(log(batchB2$OUTCOME))$p.value, shapiro.test(log(batchB3$OUTCOME))$p.value,
                                      shapiro.test(log(batchB4$OUTCOME))$p.value, shapiro.test(log(batchB5$OUTCOME))$p.value,
                                      shapiro.test(log(batchB6$OUTCOME))$p.value, shapiro.test(log(batchB7$OUTCOME))$p.value))
xtable(sw_log_frame, digits=c(0,0,4))
@
None of the batches reject normality, indicating that the Shapiro-Wilk test cannot be used to reject normality. 
We can generate a normal QQ-plot to get more insight into the log transformed batch 0.
<<figure3, fig.cap="QQ Plot of the log of batch 0", fig.show="asis", fig.pos = "h!", echo=FALSE>>=
qqnorm(log(batchB0$OUTCOME))
qqline(log(batchB0$OUTCOME))
@
\FloatBarrier
\subsection{Skewness and Kurtosis}
After performing the Shapiro-Wilk test, we have performed skewness and kurtosis tests, to get some insights in what way the transformed data follows normality. In most cases the skewness is a bit too high, while in other cases the kurtosis is not in the range of 
<<echo=FALSE, warning=FALSE, message=FALSE, results="asis">>=
library(e1071)

batches = unique(dataset$BATCH);
skewness = list();
kurtosis = list();

for(batch in batches) {
  Bi = dataset[dataset$BATCH == batch,]$LOGOUTCOME
  skewness[batch] <-skewness(Bi, type=2);
  kurtosis[batch] <-kurtosis(Bi, type=2);
}
kurtosis_skewness = data.frame(
                              "kurtosis"= unlist(kurtosis), "skewness" = unlist(skewness))
library(xtable)
xtable(kurtosis_skewness, digits = c(0,3,3))
@
\FloatBarrier
<<echo=FALSE, warning=FALSE, message=FALSE, results="asis">>=
library(moments)

batches = unique(dataset$BATCH)
jarqueje = list();

for(batch in batches) {
  Bi = dataset[dataset$BATCH == batch,]$LOGOUTCOME
  jarqueje[batch] <- as.numeric(unlist(jarque.test(Bi))["p.value"])
}
jarque_bera = data.frame(
                              "Jarque-Bera"= unlist(jarqueje))
library(xtable)
xtable(jarque_bera, digits=c(0,4))
@

We can compute the skewness and kurtosis metrics and observe that for batch 0 they are respectively. This means that although the kurtosis is in the appropriate range to be normally distributed, the skewness of the data is a bit too high. This kind of test is also formalized in the Jarque-Bera test, which tests if the skewness is close to zero and the kurtosis close to three. The p-value of the Jarque-Bera test is , so it cannot reject the normality hypothesis based on skewness and kurtosis. The data is skewed right, which is probably the reason that the Shapiro-Wilk test rejects the assumption of a normal distribution. The p-value of the Shapiro-Wilk test is not very low and the skewness, kurtosis and Jarque-Bera tests seem to suggest a close-to-normal shape. In combination with the fact that for all the other batches we cannot reject the normality null hypothesis, we will assume that batch 0 is normally distributed for the following tests. It must also be noted that the t-test is robust against data that deviates somewhat from normality, so our assumption at least for that test is reasonable.
\subsubsection{T-Test}
To see which t-test variant we need to perform, we need to analyse the variance of all the log-transformed batches.
<<echo=FALSE, warning=FALSE, message=FALSE, results="asis">>=
var_log_B7 = var(log(batchB7$OUTCOME[!is.nan(batchB7$OUTCOME)]))
variance_log_frame = data.frame(batch=c("B0", "B1", "B2", "B3", "B4", "B5", "B6", "B7"), 
                              "variance"=c(var(log(batchB0$OUTCOME)), var(log(batchB1$OUTCOME)), var(log(batchB2$OUTCOME)), var(log(batchB3$OUTCOME)),
                                           var(log(batchB4$OUTCOME)), var(log(batchB5$OUTCOME)), var(log(batchB6$OUTCOME)), var_log_B7))
library(xtable)
<<<<<<< HEAD
xtable(variance_log_frame,digits=c(0,3,3))
=======
xtable(variance_log_frame, digits=c(0,0,4))
>>>>>>> origin/master
@
The variance differs between batches, as can be seen in the preceding table. Because of that, we will perform a t-test that assumes unequal variances.Batch seven has one missing value, which should not be a problem for a t-test.\\
The results of the t-tests can be found in the following table:
<<echo=FALSE, warning=FALSE, message=FALSE, results="asis">>=
ttest_B1 <- t.test(log(batchB0$OUTCOME), log(batchB1$OUTCOME), var.equal = FALSE, alternative = "less")$p.value
ttest_B2 <- t.test(log(batchB0$OUTCOME), log(batchB2$OUTCOME), var.equal = FALSE,alternative = "less")$p.value
ttest_B3 <- t.test(log(batchB0$OUTCOME), log(batchB3$OUTCOME), var.equal = FALSE, alternative = "less")$p.value
ttest_B4 <- t.test(log(batchB0$OUTCOME), log(batchB4$OUTCOME), var.equal = FALSE, alternative = "less")$p.value
ttest_B5 <- t.test(log(batchB0$OUTCOME), log(batchB5$OUTCOME), var.equal = FALSE, alternative = "less")$p.value
ttest_B6 <- t.test(log(batchB0$OUTCOME), log(batchB6$OUTCOME), var.equal = FALSE, alternative = "less")$p.value
ttest_B7 <- t.test(log(batchB0$OUTCOME), log(batchB7$OUTCOME), var.equal = FALSE, alternative = "less")$p.value
ttest_log_frame = data.frame(batch=c("B1", "B2", "B3", "B4", "B5", "B6", "B7"), 
                              "p-value"=c(ttest_B1, ttest_B2, ttest_B3, ttest_B4, ttest_B5, ttest_B6, ttest_B7))
library(xtable)
xtable(ttest_log_frame, digits=c(0,0,4))
@
The results indicate that batch three has a mean that is significanlty different from the mean of the unaffected batch, since its p-value is less than $0.01$. This could indicate that this batch is affected.
\subsubsection{Homogeneity tests}
Performing the Barlett's tests gives us a p-value of $\Sexpr{as.numeric(unlist(bartlett.test(dataset$OUTCOME~dataset$BATCH))["p.value"])} < 0.01$. This rejects the null hypothesis that all the batches have the same variance. After performing an F-test that compares all the batches with batch 0, we have found the following results:
<<echo=FALSE, warning=FALSE, message=FALSE, results="asis">>=
vartestvalues = list()
for (batch in batches) {
  Bi = log(dataset[dataset$BATCH == batch,]$OUTCOME)
  vartestvalues[batch] <- var.test(log(batchB0$OUTCOME),Bi)$p.value
}
vartestvalues <- vartestvalues[-vartestvalues$B0]
var_log_frame = data.frame("F-test"=unlist(vartestvalues))
library(xtable)
xtable(var_log_frame,digits=c(0,3))
@
These results indicate that batch four has different properties from the other batches.
\subsection{Non-parametric tests}
\subsubsection{Serial correlation tests with means}
We have performed a Rank von Neumann Test for lag-1. The p-value is $0.490 > 0.01$ and we therefore cannot reject the null hypothesis. This indicates that no significant pattern can be found. This could indicate that the silicones have affected only one or two batches and not all batches from the point the leakage has started. 
\subsubsection{Wald-Wolfowitz Runs Test}
<<echo=FALSE, message=FALSE, warning=FALSE, results="hide">>=
library(adehabitatLT)
wawo <- wawotest(dataset$OUTCOME)
@
We have performed a Wald-Wolfowitz Runs test. We see that the p-value $=\Sexpr{wawo["p"]} < 0.01$ so we can reject that null hypothesis. This is an interesting result, because we know the data is obtained from the same source. This indeed indicates that some of the outcomes \\
\subsubsection{Rank Serial Correlation Test}
Here we perform the rank serial autocorrelation test at lag 1 using the rank von Neumann ratio. This tests the null hypothesis that the lag-$k$ autocorrelation is $0$ for all values of $k$ greater than $0$ (i.e., the time series is purely random). The procedure that we use emits some warnings because the dataset contains ties, and with this specific test that can make the p-value less accurate. With that warning in mind, we present the table containing the rank serial autocorrelation test p-values of each batch:\\
<<echo=FALSE, warning=FALSE, message=FALSE, results="asis">>=
library(EnvStats)
serialCorB0 = unlist(serialCorrelationTest(batchB0$OUTCOME[!is.nan(batchB0$OUTCOME)], test = "rank.von.Neumann")["p.value"])
serialCorB1 = unlist(serialCorrelationTest(batchB1$OUTCOME[!is.nan(batchB1$OUTCOME)], test = "rank.von.Neumann")["p.value"])
serialCorB2 = unlist(serialCorrelationTest(batchB2$OUTCOME[!is.nan(batchB2$OUTCOME)], test = "rank.von.Neumann")["p.value"])
serialCorB3 = unlist(serialCorrelationTest(batchB3$OUTCOME[!is.nan(batchB3$OUTCOME)], test = "rank.von.Neumann")["p.value"])
serialCorB4 = unlist(serialCorrelationTest(batchB4$OUTCOME[!is.nan(batchB4$OUTCOME)], test = "rank.von.Neumann")["p.value"])
serialCorB5 = unlist(serialCorrelationTest(batchB5$OUTCOME[!is.nan(batchB5$OUTCOME)], test = "rank.von.Neumann")["p.value"])
serialCorB6 = unlist(serialCorrelationTest(batchB6$OUTCOME[!is.nan(batchB6$OUTCOME)], test = "rank.von.Neumann")["p.value"])
serialCorB7 = unlist(serialCorrelationTest(batchB7$OUTCOME[!is.nan(batchB7$OUTCOME)], test = "rank.von.Neumann")["p.value"])
serial_cor_frame = data.frame(batch=c("B0", "B1", "B2", "B3", "B4", "B5", "B6", "B7"), 
                              "p-value"=c(serialCorB0, serialCorB1, serialCorB2, serialCorB3,
                                          serialCorB4, serialCorB5, serialCorB6, serialCorB7))
library(xtable)
xtable(serial_cor_frame, digits=c(0,0,4))
@
As one can observe, the p-values are all large, which means that we cannot reject the null hypothesis that the batches are random using this test.

\subsubsection{Kolmogorov-Smirnov Test}
The results of the Kolmogorov-Smirnov test can be found in the following table. 
<<echo=FALSE, warning=FALSE, message=FALSE, results="asis">>=
ksB1 = unlist(ks.test(batchB0$OUTCOME, batchB1$OUTCOME, exact=TRUE)["p.value"])
ksB2 = unlist(ks.test(batchB0$OUTCOME, batchB2$OUTCOME, exact=TRUE)["p.value"])
ksB3 = unlist(ks.test(batchB0$OUTCOME, batchB3$OUTCOME, exact=TRUE)["p.value"])
ksB4 = unlist(ks.test(batchB0$OUTCOME, batchB4$OUTCOME, exact=TRUE)["p.value"])
ksB5 = unlist(ks.test(batchB0$OUTCOME, batchB5$OUTCOME, exact=TRUE)["p.value"])
ksB6 = unlist(ks.test(batchB0$OUTCOME, batchB6$OUTCOME, exact=TRUE)["p.value"])
ksB7 = unlist(ks.test(batchB0$OUTCOME, batchB7full$OUTCOME, exact=TRUE)["p.value"])
ksB7m = unlist(ks.test(batchB0$OUTCOME, batchB7mean$OUTCOME, exact=TRUE)["p.value"])

ks_frame = data.frame(batch=c("B1", "B2", "B3", "B4", "B5", "B6", "B7", "B7*"), 
                              "p-value"=c(ksB1, ksB2, ksB3, ksB4, ksB5, ksB6, ksB7, ksB7m))

library(xtable)
xtable(ks_frame, digits=c(0,0,4))
@

None of the batches violate the null-hypothesis, since all p-values are larger than $0.01$. Note that the missing value in B7 is omitted in the test with B7 and replaced with the mean of all other values in B7 in the test with B7*. 

\subsubsection{Wilcoxon Signed-Rank Test}

<<echo=FALSE, warning=FALSE, message=FALSE, results="asis">>=
wilcoxB1 = unlist(wilcox.test(batchB0$OUTCOME, batchB1$OUTCOME, paired=TRUE, correct=FALSE, exact=TRUE, alternative="less")["p.value"])
wilcoxB2 = unlist(wilcox.test(batchB0$OUTCOME, batchB2$OUTCOME, paired=TRUE, correct=FALSE, exact=TRUE, alternative="less")["p.value"])
wilcoxB3 = unlist(wilcox.test(batchB0$OUTCOME, batchB3$OUTCOME, paired=TRUE, correct=FALSE, exact=TRUE, alternative="less")["p.value"])
wilcoxB4 = unlist(wilcox.test(batchB0$OUTCOME, batchB4$OUTCOME, paired=TRUE, correct=FALSE, exact=TRUE, alternative="less")["p.value"])
wilcoxB5 = unlist(wilcox.test(batchB0$OUTCOME, batchB5$OUTCOME, paired=TRUE, correct=FALSE, exact=TRUE, alternative="less")["p.value"])
wilcoxB6 = unlist(wilcox.test(batchB0$OUTCOME, batchB6$OUTCOME, paired=TRUE, correct=FALSE, exact=TRUE, alternative="less")["p.value"])
wilcoxB7 = unlist(wilcox.test(batchB0$OUTCOME, batchB7full$OUTCOME, paired=TRUE, correct=FALSE, exact=TRUE, alternative="less")["p.value"])
wilcoxB7m = unlist(wilcox.test(batchB0$OUTCOME, batchB7mean$OUTCOME, paired=TRUE, correct=FALSE, exact=TRUE, alternative="less")["p.value"])

wilcox_frame = data.frame(batch=c("B1", "B2", "B3", "B4", "B5", "B6", "B7", "B7*"), 
                              "p-value"=c(wilcoxB1, wilcoxB2, wilcoxB3, wilcoxB4, wilcoxB5, wilcoxB6, wilcoxB7, wilcoxB7m))

library(xtable)
xtable(wilcox_frame, digits=c(0,0,4))
@

B2, B3 and B7, test as potentially tainted if we take $\alpha =0.025$. That is, it is unlikely that the higher sample values from these batches are due to sample bias. Applying relaxed Bonferroni correction ($p<0.0045$), however, results in the null hypthesis holding for all potentially tainted batches+ yet again. 

\section{Conclusion}
\end{document}
