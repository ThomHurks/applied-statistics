\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{placeins}
\usepackage{longtable,framed}
\usepackage{amsmath,amssymb}

<<echo=FALSE, message=FALSE, results='hide'>>=
library(sas7bdat)
dataset = read.sas7bdat("assignment1.sas7bdat")
dataset = dataset[!is.nan(dataset$OUTCOME),]
dataset$LOGOUTCOME = log(dataset$OUTCOME)
dataset = dataset[!is.nan(dataset$OUTCOME),]
batchB0 = dataset[dataset$BATCH == "B0",]
batchB1 = dataset[dataset$BATCH == "B1",]
batchB2 = dataset[dataset$BATCH == "B2",]
batchB3 = dataset[dataset$BATCH == "B3",]
batchB4 = dataset[dataset$BATCH == "B4",]
batchB5 = dataset[dataset$BATCH == "B5",]
batchB6 = dataset[dataset$BATCH == "B6",]
batchB7 = dataset[dataset$BATCH == "B7",]

all_batches = unique(dataset$BATCH)
batches = all_batches[all_batches != "B0"]
@

\begin{document}
\title{Analysis of silicone contamination of batches}

\author{
\begin{minipage}{0.6\textwidth}
\centering
\begin{tabular}{l r}
Thom Hurks & \textit{0828691}\\
Puck Mulders & \textit{0737709}\\
Marijn van Knippenberg  & \textit{0676548}\\
Rik Coenders & \textit{0777004}
\end{tabular}
\end{minipage}
}

\date{\today}

\maketitle
\newpage
\tableofcontents
\newpage
\section{Introduction}
The health inspection has detected a silicone leakage in a machine used in a chemical production process. The leakage was detected in close vicinity of packed product, so the question is whether the product has been affected by the leakage. The machines are regularly inspected, and data on a batch produced before the previous inspection is available and is denoted as batch B0. Batches denoted B1 to B7 have been produced between the previous inspection and the current inspection and may have been affected by the leakage. The data consists of measurements of samples of units from the batches. The silicone that leaked is already part of the product itself, so it will always be present in the measured units to some extent. The goal of this report is to possibly identify the batches that were affected by the silicone leakage. If a batch is affected, we expect the amount of silicone to be higher compared to batch B0 and other unaffected batches. We therefore hypothesize that the affected batches are location shifted such that the mean of the outcome is higher than that of unaffected batches. \\\\
In the first section we describe which methods we use to find the affected batches. In the next section we will describe what the results of these methods are. Lastly, we will conclude which of the batches are affected. In this section we will also describe what the weaknesses and risks are of our investigation.
\subsection{Reproducible Research}
We value the idea of reproducible research, which is publishing data analyses together with the software code, so others can reproduce and verify our findings and possibly extend our research. For this reason this report has been created using Knitr, which is a dynamic report generation tool for the statistical programming language R. The Knitr sourcecode for this report is available from the authors if others are interested in verifying our results.

\newpage
\section{Methods}
To decide if any of the batches are affected, we perform both parametric and non-parametric tests. The parametric tests assume that the data is normally distributed, which means we must test for normality and, if necessary, transform the data. Non-parametric tests are generally less powerful than parametric tests, but can be applied regardless of the measure of normality. With parametric tests one must be careful to draw conclusions about the original non-transformed data if the data needs to be transformed in order to meet the normality assumption. 
\subsection{Exploratory data analysis}
Before performing any parametric or non-parametric tests, we will first perform some standard statistical tests to gain insight into the data. As a start, we will plot the data and calculate the mean, standard deviation, variance and median of each batch. Additionally, we will perform a Shapiro-Wilk test and an Anderson-Darling test to see whether the non-transformed data is normal. We need both of these tests, since Shapiro-Wilk is more powerful than Anderson-Darling, but cannot handle ties as well as Anderson-Darling. We have formulated the null hypothesis of both tests as can be found in equation \ref{normal}:
\begin{align}
\label{normal}
\begin{cases}
H_0: B_i \sim \mathcal{N} & \quad \text{for } 1 \leq i \leq 7 \\
H_1: B_i\nsim \mathcal{N}  \\
\end{cases}
\end{align} 
We will reject the null hypothesis if the p-value $< 0.01$. 
After that, we will perform a Wald-Wolfowitz Runs Test which has as a null hypothesis that each element in the sequence is independently drawn from the same random distribution which is expected if none of the batches are affected by silicones. We reject the null hypothesis if $p < 0.01$.
\subsection{Critical value}
Most of our hypotheses include testing all seven possibly affected batches. Because of this reason, we have to adjust the commonly used critical value of $0.05$ to make sure we do not reject the null hypothesis due to chance. Applying Bonferroni correction gives us a critical value of $\alpha=\frac{0,05}{7}=0,00714$, but since Bonferroni correction is quite conservative we may fail to reject the null hypotheses in cases where we actually want to do so; our goal is to identify the affected batches, so we can be a bit more flexible and raise the alpha value slightly. As such, we will set the critical value to $0.01$.\\
Some tests, however, test data from all batches in one go and for these tests the Bonferroni correction is not necessary. For those tests we will use the original critical value of $\alpha=0.05$.
\subsection{Parametric tests}
\subsubsection{Investigating normality on transformed data}
Some statistical tests assume normality of the data, or require the data to at least approach normality. For that reason we will investige if the data is normally distributed by inspecting normal QQ-plots and by using two tests on noramlity: the Shapiro-Wilk test and Anderson-Darling. If necessary we will transform the data in such a way that the transformed data is normally distributed or nearly normal. For non-parametric tests that do not assume any specific distribution we will use the original non-transformed data. The null hypothesis of both Shapiro-Wilk and Anderson-Darling can be found in equation \ref{normal}. We will reject the null hypothesis if the p-value $<0.01$.
Besides the Anderson-Darling and Shapiro-Wilk test, we will perform skwewness and kurtosis tests to determine where normality can be rejected. The critical value of a two-sided skewness test is 1.157, while the critical value of the kurtosis is 1.49. We will perform a Jarque-Bera test to see if it is appropriate to reject a normal distribution based on skewness and kurtosis. The null hypothesis is the same as with the Shapiro-Wilk and Anderson-Darling test which can be found in equation \ref{normal}. We will reject normality if the p-value $< 0.01$.
\subsubsection{T-test}
Because batch 0 is produced before the previous inspection where no leakage was discovered, we know that the distribution of the amount of silicones found in each batch should be similar to the amount of silicone in batch 0. To check whether they are similar, we will perform a one-sided t-test. We have chosen a one-sided test, since a leakage in silicones will cause the amount to be higher in the batches and thus we have to test only if the test is violated on one side. To perform a proper t-test, we have to know if the variances of the batches are equal. We check this by applying Barlett's test, which checks whether all the variances of the batches are equal to each other. This can be formulated as can be found in \ref{barlet}:
\begin{align}
\label{barlet}
\begin{cases}
H_0: \sigma_0 = \sigma_1 = \sigma_2 ... = \sigma_7  \\
H_1: \sigma_0 \neq \sigma_1 \neq \sigma_2 ... \neq \sigma_7
\end{cases}
\end{align}
We will reject the null hypothesis if p-value $< 0.05$. Based on these results, we will apply either a equal or unequal t-test. The hypothesis of the t-test is the following:
\begin{align}
\label{means}
\begin{cases}
H_0: \mu_0 \geq \mu_i & \quad \text{for } 1 \leq i \leq 7 \\
H_1: \mu_0 < \mu_i
\end{cases}
\end{align}
If the result of the t-test is less than $0.01$, we will reject the null hypothesis.
\subsubsection{Variance testing}
Besides performing a t-test, we will also perform some homogeneity tests to see whether the data also share the same characteristics. We will perform an F-test to see which batches have a variance similiar to batch 0. This can be formulated as can be found in equation \ref{ftest}:
\begin{align}
\label{ftest}
\begin{cases}
H_0: \sigma_0 = \sigma_i & \quad \text{for } 1 \leq i \leq 7 \\
H_1: \sigma_0 \neq \sigma_i
\end{cases}
\end{align}
We will reject the null hypothesis if the p-value $<0.01$.
\subsubsection{Outliers}
We will perform a Grubbs outlier test on the log-transformed to check whether there are any indications that one of the measurements has some incorrect value due to measurement errors, for example. We will do this for both outliers on the left and right tail. Our null hypothesis is can be found in equation \ref{outlier}.We will reject the null hypothesis if the p-value $< 0.01$.
\begin{align}
\label{outlier}
\begin{cases}
H_0: \text{Batch}_i \text{ contains no outliers} & \quad \text{for } 1 \leq i \leq 7 \\
H_1: \text{Batch}_i \text{ contains outliers} \\
\end{cases}
\end{align}
We will reject the null hypothesis if the p-value$<0.01$.
\subsection{Non-parametric tests}
\subsubsection{Kolmogorov-Smirnov Test}
If the non-transformed data does not follow a normal distribution, we will also perform some non-parametric tests that do not assume a normal distribution. Such a parametric test is the Kolgomorov-Smirnov test. For the two-sample version of this test, we need to make the following assumptions:
\begin{itemize}
  \item The two samples are independent.
  \item The outcomes are ordinal or numerical.
\end{itemize}
Since the results of the samples do not depend on each other, the first assumption can be made. Second, the outcomes are all numerical so the second assumption can be made too. We perform a two-sided test to get a general view of whether non-parametric testing picks up on any differences between the distributions of batch 0 and the other batches. Our hypothesis is the following:
We will reject the null hypothesis when $p < 0.01$. The Kolmogorov-Smirnov test will not give the exact p-value due to ties in the data, so it could be the case that the p-value resulting from the test somewhat deviates from the real p-value. Because we have picked a somewhat high critical value, some bias of p-values can be handled.
\subsubsection{Wilcoxon Rank-Sum Test}
A second non-parametric test we will perform is the Wilcoxon Rank-Sum Test, which needs the additional assumption that both distributions are from an ordinal distribution. This implies that the data cannot contain any ties. Since we have some ties in the data, we will not get the exact p-value. Since our value does not contain a substantial amount of ties, the bias of the p-value will not be extremely high. As with the Kolmogorov-Smirnov test, we think that our critical value can handle some bias of the p-values calculated by the Wilcoxon Rank-Sum Test.
The hypotheses of the Wilcoxon Rank-Sum test can be found in equation \ref{wilcox}:
\begin{align}
\label{wilcox}
\begin{cases}
H_0: \text{Batch}_0 \text{ and batch}_i \text{ come from the same population} & \quad \text{for } 1 \leq i \leq 7 \\
H_1: \text{Batch}_0 \text{ and batch}_i \text{ come from a different population} 
\end{cases}
\end{align}
This null hypothesis indicates that the change that the median of $\text{batch}_0$ is larger than the median of $\text{batch}_1$ is the same as that the median of $\text{batch}_0$ is smaller than the median of $\text{batch}_1$. We will reject the null hypothesis if p-value $< 0.01$. 
\newpage
\section{Results}
\subsection{Exploratory data analysis}
The results of the standard data analysis can be found in the table below.
<<echo=FALSE, results='asis'>>=
means = list()
variances = list()
medians = list()
standevs = list()
for (batch in all_batches) {
  cur_batch = dataset[dataset$BATCH == batch,]$OUTCOME
  means[batch] = mean(cur_batch)
  medians[batch] = median(cur_batch)
  variances[batch] = var(cur_batch)
  standevs[batch] = sd(cur_batch)
}
metrics_frame = data.frame("mean" = unlist(means),
                           "median" = unlist(medians),
                           "variance" = unlist(variances),
                           "st-dev" = unlist(standevs))
library(xtable)
xtable(metrics_frame)
@
\FloatBarrier
We also discovered that batch seven has a missing value ("NA"). Some of the statistical tests that we plan to perform do not require that the number of data points between batches is the same, and since there is only a single missing value in the whole dataset, we can simply remove the missing value in those cases. In other cases the missing value will be replaced by the mean of the other sample values in batch 7. 
The next step is to generate a normal QQ plot for batch 0 as an easy visual confirmation of normality.
<<figure1, fig.cap="QQ Plot of batch 0", fig.show="asis", fig.pos = "h!", echo=FALSE>>=
qqnorm(batchB0$OUTCOME)
qqline(batchB0$OUTCOME)
swtestB0 = shapiro.test(batchB0$OUTCOME)
@
\FloatBarrier
The normal QQ plot already seems to suggest that batch 0 is not normally distributed. When we also perform the Shapiro-Wilk normality test we can see the p-value $= \Sexpr{swtestB0$p.value} < 0.01$ so we can reject the null-hypothesis that the outcome is normally distributed.\\\\
Next we can generate a normal QQ plot for all batches 0 to 7. Of course generating this plot for multiple batches at once assumes that the units are somewhat consistent across different batches.
<<figure2, fig.cap="QQ Plot of batches 0 to 7", fig.show="asis", fig.pos = "h!", echo=FALSE>>=
qqnorm(dataset$OUTCOME)
qqline(dataset$OUTCOME)
@
\FloatBarrier
The normal QQ plot here too seems to suggest that the data is not normally distributed. We can again compute the Shapiro-Wilk test p-value for all batches to gather more information.
<<echo=FALSE, results='asis'>>=
shapiro_values = list()
for (batch in all_batches) {
  shapiro_values[batch] = shapiro.test(dataset[dataset$BATCH == batch,]$OUTCOME)$p.value
}
shapiro_frame = data.frame("p-value"=unlist(shapiro_values))
library(xtable)
xtable(shapiro_frame, digits=c(0,4))
@
\FloatBarrier
We can reject the normality null hypothesis for batches zero, five and six with p-values respectively $\Sexpr{shapiro_frame["B0",]}$, $\Sexpr{shapiro_frame["B5",]}$ and $\Sexpr{shapiro_frame["B6",]}$. We can also observe that for the other batches the p-values are not particularly high, the exception being batch 3 with a p-value of $\Sexpr{shapiro_frame["B3",]}$.\\\\
If we perform the Anderson-Darling test, we see that only batch three, four and seven follow a normal distribution. On all the other batches, normality is rejected with a very low p-value. 
<<echo=FALSE, results='asis'>>=
library(nortest)
anderson_values = list()
for (batch in all_batches) {
  anderson_values[batch] = ad.test(dataset[dataset$BATCH == batch,]$OUTCOME)$p.value
}
anderson_frame = data.frame("p-value"=unlist(anderson_values))
library(xtable)
xtable(anderson_frame, digits=c(0,4))
@
\FloatBarrier
\subsection{Parametric tests}
\subsubsection{Shapiro-Wilk on log-transformed data}
In the previous section we observed that for most batches, including the batch 0 which we want to compare with the other batches. In order to perform parametric tests that assume normality, we can transform the data such that all batches are normally distributed or close to a normal distribution. We will transform the data using the well-known log transformation and then check for each batch what the p-value of the Shapiro-Wilk test is in order to confirm that the transformation had the desired effect.
<<echo=FALSE, results="asis">>=
sw_log_frame = data.frame(batch=c("B0", "B1", "B2", "B3", "B4", "B5", "B6", "B7"), 
                          "p-value"=c(shapiro.test(batchB0$LOGOUTCOME)$p.value,
                                      shapiro.test(batchB1$LOGOUTCOME)$p.value,
                                      shapiro.test(batchB2$LOGOUTCOME)$p.value,
                                      shapiro.test(batchB3$LOGOUTCOME)$p.value,
                                      shapiro.test(batchB4$LOGOUTCOME)$p.value,
                                      shapiro.test(batchB5$LOGOUTCOME)$p.value,
                                      shapiro.test(batchB6$LOGOUTCOME)$p.value,
                                      shapiro.test(batchB7$LOGOUTCOME)$p.value))
library(xtable)
xtable(sw_log_frame, digits=c(0,0,4))
@
\FloatBarrier
None of the batches reject normality, indicating that the Shapiro-Wilk test cannot be used to reject normality for the transformed batches.
\subsubsection{Anderson-Darling on log-transformed data}
We have also performed the Anderson-Darling test to investigate if the transformation causes the data to be more normally shaped.
<<echo=FALSE, results='asis'>>=
library(nortest)
anderson_trans_values = list()
for (batch in all_batches) {
  anderson_values[batch] = ad.test(dataset[dataset$BATCH == batch,]$LOGOUTCOME)$p.value
}
anderson_trans_frame = data.frame("p-value"=unlist(anderson_trans_values))
library(xtable)
xtable(anderson_trans_frame, digits=c(0,4))
@
Again, none of the batches reject normality.
We can generate a normal QQ-plot to get more insight into the log transformed batch 0.
<<figure3, fig.cap="QQ Plot of the log of batch 0", fig.show="asis", fig.pos = "h!", echo=FALSE>>=
qqnorm(log(batchB0$OUTCOME))
qqline(log(batchB0$OUTCOME))
@
\FloatBarrier
\subsection{Skewness and Kurtosis}
After performing the Shapiro-Wilk and Anderson-Darling tests, we perform skewness and kurtosis tests to get some insight in the way the log-transformed data follows normality. In most batches the skewness seems a bit too far away from zero, while in some batches the kurtosis is away from three.
<<echo=FALSE, warning=FALSE, message=FALSE, results="asis">>=
library(e1071)
skewness_batches = list();
kurtosis_batches = list();
for (batch in all_batches) {
  Bi = dataset[dataset$BATCH == batch,]$LOGOUTCOME
  skewness_batches[batch] <- e1071::skewness(Bi, type=2)
  kurtosis_batches[batch] <- e1071::kurtosis(Bi, type=2)
}
kurtosis_skewness = data.frame("kurtosis"= unlist(kurtosis_batches), "skewness" = unlist(skewness_batches))
library(xtable)
xtable(kurtosis_skewness, digits = c(0,3,3))
@
\FloatBarrier
Kurtosis and skewness tests are also formalized in the Jarque-Bera test, which tests if the skewness is close to zero and the kurtosis close to three. The p-value of the Jarque-Bera test is above $0.01$ for all batches, so we cannot reject the normality hypothesis based on skewness and kurtosis. Not only do the Shapiro-Wilk and Anderson-Darling test indicate that assuming normality on the log-transformed data is not rejected, but also Jarque-Bera indicates this is valid. Therefore the normality assumption for the transformed data is reasonable for the following tests. It must also be noted that the t-test is robust against data that deviates somewhat from normality, so our assumption at least for that test seems to be very reasonable.
<<echo=FALSE, warning=FALSE, message=FALSE, results="asis">>=
library(moments)
batches_jarque = list()
for (batch in all_batches) {
  Bi = dataset[dataset$BATCH == batch,]$LOGOUTCOME
  batches_jarque[batch] <- as.numeric(unlist(jarque.test(Bi))["p.value"])
}
jarque_frame = data.frame("Jarque-Bera"= unlist(batches_jarque))
library(xtable)
xtable(jarque_frame, digits=c(0,4))
@
\FloatBarrier
\subsubsection{T-Test}
Performing the Barlett's tests gives us a p-value of $\Sexpr{as.numeric(unlist(bartlett.test(log(dataset$OUTCOME)~dataset$BATCH))["p.value"])} < 0.05$. This rejects the null hypothesis that all the batches have equal variance. This implies that we have to perform a t-test that assumes unequal variances. Batch seven has one missing value, which should not be a problem for a t-test.\\
The results of the t-tests can be found in the following table:
<<echo=FALSE, warning=FALSE, message=FALSE, results="asis">>=
ttests_batches = list()
for (batch in batches) {
  ttests_batches[batch] <- t.test(batchB0$LOGOUTCOME, dataset[dataset$BATCH == batch,]$LOGOUTCOME,
                                  var.equal = FALSE, alternative = "less")$p.value
}
ttest_log_frame = data.frame("p-value"=unlist(ttests_batches))
library(xtable)
xtable(ttest_log_frame, digits=c(0,4))
@
\FloatBarrier
The results indicate that batch three has a mean that is significanlty different from the mean of the unaffected batch, since its p-value is less than $0.01$. This could indicate that this batch is affected. It must also be noted that the hypothesis for batches two and seven is very near being rejected as well.
\subsubsection{Variance testing}
After performing an F-test that compares all the batches with batch 0, we have found the following results:
<<echo=FALSE, warning=FALSE, message=FALSE, results="asis">>=
vartestvalues = list()
for (batch in batches) {
  vartestvalues[batch] <- var.test(batchB0$LOGOUTCOME, dataset[dataset$BATCH == batch,]$LOGOUTCOME)$p.value
}
var_log_frame = data.frame("F-test"=unlist(vartestvalues))
library(xtable)
xtable(var_log_frame,digits=c(0,3))
@
\FloatBarrier
These results indicate that batch four has a significantly different variance than the other batches.
\subsubsection{Outliers}
<<echo=FALSE, warning=FALSE, message=FALSE, results="asis">>=
library(outliers)
grubbsMin = list(`min`=c(),`p-value`=c());
grubbsMax = list(`max`=c(),`p-value`=c());
for(batch in batches) {
  Bi = dataset[dataset$BATCH == batch,]$OUTCOME;
  logBi = dataset[dataset$BATCH == batch,]$LOGOUTCOME;
  grubbsMin$min[batch] = min(Bi);
  grubbsMax$max[batch] = max(Bi);
  grubbsMin$`p-value`[batch] = grubbs.test(logBi, type=10, opposite=TRUE)$p.value;
  grubbsMax$`p-value`[batch] = grubbs.test(logBi, type=10)$p.value;
}
library(xtable);
xtable(as.data.frame(grubbsMin));
@
\FloatBarrier
<<echo=FALSE, warning=FALSE, message=FALSE, results="asis">>=
library(xtable);
xtable(as.data.frame(grubbsMax));
@
\FloatBarrier
Both the extreme on the left and the right tail are not significantly big to reject them as an outlier. This indicates that no values can be indicated as errors due mistakes that people maked while measuring the amount of $\mu$g in the batches.
\subsection{Non-parametric tests}
\subsubsection{Wald-Wolfowitz Runs Test}
<<echo=FALSE, message=FALSE, warning=FALSE, results="hide">>=
library(adehabitatLT)
wawo_p <- wawotest(dataset$OUTCOME)["p"]
@
We have performed a Wald-Wolfowitz Runs test on the whole original data for which the null hypothesis is that the data is randomly drawn from the same distribution. We see that the p-value $=\Sexpr{wawo_p} < 0.05$ so we can reject that null hypothesis. This is an interesting result, because we know the data is obtained from the same source.
\subsubsection{Kolmogorov-Smirnov Test}
The results of the Kolmogorov-Smirnov test can be found in the following table. 
<<echo=FALSE, warning=FALSE, message=FALSE, results="asis">>=
ksB1 =  unlist(ks.test(batchB0$OUTCOME, batchB1$OUTCOME,         exact=TRUE)["p.value"])
ksB2 =  unlist(ks.test(batchB0$OUTCOME, batchB2$OUTCOME,         exact=TRUE)["p.value"])
ksB3 =  unlist(ks.test(batchB0$OUTCOME, batchB3$OUTCOME,         exact=TRUE)["p.value"])
ksB4 =  unlist(ks.test(batchB0$OUTCOME, batchB4$OUTCOME,         exact=TRUE)["p.value"])
ksB5 =  unlist(ks.test(batchB0$OUTCOME, batchB5$OUTCOME,         exact=TRUE)["p.value"])
ksB6 =  unlist(ks.test(batchB0$OUTCOME, batchB6$OUTCOME,         exact=TRUE)["p.value"])
ksB7 =  unlist(ks.test(batchB0$OUTCOME, batchB7$OUTCOME,         exact=TRUE)["p.value"])
ks_frame = data.frame(batch=c("B1", "B2", "B3", "B4", "B5", "B6", "B7"), 
                              "p-value"=c(ksB1, ksB2, ksB3, ksB4, ksB5, ksB6, ksB7))

library(xtable)
xtable(ks_frame, digits=c(0,0,4))
@
\FloatBarrier
None of the batches violate the null-hypothesis that the samples are independently and randomly drawn from the same distribution, since all p-values are larger than $0.01$. However, we can observe that the p-values for batch three and five are low. Note that the Kolmogovor-Smirnov test is a stronger test than the Wald-Wolfowitz test to detect differences between distributions that differ in location.
\subsubsection{Wilcoxon Rank-Sum Test}

<<echo=FALSE, warning=FALSE, message=FALSE, results="asis">>=
wilcoxB1 = unlist(wilcox.test(batchB0$OUTCOME, batchB1$OUTCOME,
                              paired=FALSE, correct=FALSE, exact=TRUE, alternative="less")["p.value"])
wilcoxB2 = unlist(wilcox.test(batchB0$OUTCOME, batchB2$OUTCOME,
                              paired=FALSE, correct=FALSE, exact=TRUE, alternative="less")["p.value"])
wilcoxB3 = unlist(wilcox.test(batchB0$OUTCOME, batchB3$OUTCOME,
                              paired=FALSE, correct=FALSE, exact=TRUE, alternative="less")["p.value"])
wilcoxB4 = unlist(wilcox.test(batchB0$OUTCOME, batchB4$OUTCOME,
                              paired=FALSE, correct=FALSE, exact=TRUE, alternative="less")["p.value"])
wilcoxB5 = unlist(wilcox.test(batchB0$OUTCOME, batchB5$OUTCOME,
                              paired=FALSE, correct=FALSE, exact=TRUE, alternative="less")["p.value"])
wilcoxB6 = unlist(wilcox.test(batchB0$OUTCOME, batchB6$OUTCOME,
                              paired=FALSE, correct=FALSE, exact=TRUE, alternative="less")["p.value"])
wilcoxB7 = unlist(wilcox.test(batchB0$OUTCOME, batchB7$OUTCOME,
                              paired=FALSE, correct=FALSE, exact=TRUE, alternative="less")["p.value"])

wilcox_frame = data.frame(batch=c("B1", "B2", "B3", "B4", "B5", "B6", "B7"), 
                          "p-value"=c(wilcoxB1, wilcoxB2, wilcoxB3, wilcoxB4, wilcoxB5, wilcoxB6,
                                      wilcoxB7))

library(xtable)
xtable(wilcox_frame, digits=c(0,0,4))
@
\FloatBarrier
We can observe that the p-value for batch three $= \Sexpr{wilcoxB3} < 0.01$ and so for batch three we reject the null hypothesis that the population mean ranks are similar. The other batches do not deviate significantly from batch zero.
\newpage
\section{Conclusion}
If we consider the results, we see that batch three fails both the two-sided t-test and the Wilcoxon Rank-Sum Test. Batch three also reports a low p-value on the Kolmogorov-Smirnov test. We have hypothesized that we expect some kind of location shift of the distribution towards a higher outcome. Both the Wilcoxon Rank-Sum Test and t-test are tests that investigate whether such a shift has taken place, and by rejecting the null hypothesis they indicate that batch three indeed has shifted towards a higher amount of silicones in its batch. The Kolmogorov-Smirnov test can also be used to detect location-shifts between distributions, so a low p-value for batch three on that test may also be indicative. Because of these reasons, we advise not to allow batch three to be sold, since we strongly suspect that batch three is affected by the silicones.\\\\
Batch two and batch seven are also highly suspicious. Both batches pass the Wilcoxon Rank-Sum Test and t-test with a p-value very close to the critical value. It would be wise to do some further investigation on these two batches to get some more insight into whether the batches are contaminated.\\\\
If we consider the other tests, we see that batch four does not pass the homogeneity tests. This implies that the data from batch three has a different variance than batch 0. This is indeed the case, since the variance of batch three is quite low. Because batch four easily passes both the t-test and the Wilcoxon Rank-Sum Test we do not have any concerns that batch four is affected by silicones.\\\\
Since all the rest of the batches pass all the tests that could indicate an affection of silicons, we see no reason to state that batch one, two, four, five, six and seven are affected. Those batches should be suitable for sale.

\subsection{Weaknesses of our approach}
One of the weaknesses of our data analysis is the amount of data. Each batch has very little data, which increases the probability that the data is biased. This could mean that the comparison we have made with batch zero has been made with a highly biased data set. This would result in wrongly accepting or rejecting null hypotheses. \\\\
The other weakness of our investigation is that we have only performed some conservative tests. Because we adjusted the critical value, we increase the probability that a type II error occurs. Especially batch two and batch seven could be the victim of a type II error, so it may be useful to obtain more data of batch zero, two and seven to investigate further whether they are acceptable or affected. As an extension of the research we have already performed, it could also be useful to perform an ANOVA test to see if this yields a similar result.
We log-transformed the data in order to perform tests that assume normality. This changes the meaning of the tests with respect to the original data, which is why we also performed non-parametric tests.
If more data on the suspected batches is made available, our research can be easily repeated because of our use of the dynamic report generation tool Knitr.

\end{document}
